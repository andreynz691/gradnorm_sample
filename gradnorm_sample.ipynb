{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from timm.models import create_model\n",
    "from timm.models.helpers import load_pretrained\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from timm.models.registry import register_model\n",
    "import torch.optim as optim\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "import pickle\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _cfg(url='', **kwargs):\n",
    "    return {\n",
    "        'url': url,\n",
    "        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n",
    "        'crop_pct': .9, 'interpolation': 'bicubic',\n",
    "        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n",
    "        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n",
    "        **kwargs\n",
    "    }\n",
    "\n",
    "\n",
    "default_cfgs = {\n",
    "    'cswin_224': _cfg(),\n",
    "    'cswin_384': _cfg(\n",
    "        crop_pct=1.0\n",
    "    ),\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LePEAttention(nn.Module):\n",
    "    def __init__(self, dim, resolution, idx, split_size=7, dim_out=None, num_heads=8, attn_drop=0., proj_drop=0.,\n",
    "                 qk_scale=None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.dim_out = dim_out or dim\n",
    "        self.resolution = resolution\n",
    "        self.split_size = split_size\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        if idx == -1:\n",
    "            H_sp, W_sp = self.resolution, self.resolution\n",
    "        elif idx == 0:\n",
    "            H_sp, W_sp = self.resolution, self.split_size\n",
    "        elif idx == 1:\n",
    "            W_sp, H_sp = self.resolution, self.split_size\n",
    "        else:\n",
    "            print(\"ERROR MODE\", idx)\n",
    "            exit(0)\n",
    "        self.H_sp = H_sp\n",
    "        self.W_sp = W_sp\n",
    "        stride = 1\n",
    "        self.get_v = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1, groups=dim)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "\n",
    "        # self.out = None\n",
    "\n",
    "    def im2cswin(self, x):\n",
    "        B, N, C = x.shape\n",
    "        H = W = int(np.sqrt(N))\n",
    "        x = x.transpose(-2, -1).reshape(B, C, H, W)\n",
    "        x = img2windows(x, self.H_sp, self.W_sp)\n",
    "        x = x.reshape(-1, self.H_sp * self.W_sp, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3).contiguous()\n",
    "        return x\n",
    "\n",
    "    def get_lepe(self, x, func):\n",
    "        B, N, C = x.shape\n",
    "        H = W = int(np.sqrt(N))\n",
    "        x = x.transpose(-2, -1).reshape(B, C, H, W)\n",
    "\n",
    "        H_sp, W_sp = self.H_sp, self.W_sp\n",
    "        x = x.reshape(B, C, H // H_sp, H_sp, W // W_sp, W_sp)\n",
    "        x = x.permute(0, 2, 4, 1, 3, 5).reshape(-1, C, H_sp, W_sp)  ### B', C, H', W'\n",
    "\n",
    "        lepe = func(x)  ### B', C, H', W'\n",
    "        lepe = lepe.reshape(-1, self.num_heads, C // self.num_heads, H_sp * W_sp).permute(0, 1, 3, 2)\n",
    "\n",
    "        x = x.reshape(-1, self.num_heads, C // self.num_heads, self.H_sp * self.W_sp).permute(0, 1, 3, 2)\n",
    "        return x, lepe\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        \"\"\"\n",
    "        x: B L C\n",
    "        \"\"\"\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        ### Img2Window\n",
    "        H = W = self.resolution\n",
    "        B, L, C = q.shape\n",
    "        assert L == H * W, \"flatten img_tokens has wrong size\"\n",
    "\n",
    "        q = self.im2cswin(q)\n",
    "        k = self.im2cswin(k)\n",
    "        v, lepe = self.get_lepe(v, self.get_v)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))  # B head N C @ B head C N --> B head N N\n",
    "        attn = nn.functional.softmax(attn, dim=-1, dtype=attn.dtype)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v) + lepe\n",
    "        x = x.transpose(1, 2).reshape(-1, self.H_sp * self.W_sp, C)  # B head N N @ B head N C\n",
    "\n",
    "        ### Window2Img\n",
    "        x = windows2img(x, self.H_sp, self.W_sp, H, W).reshape(B, -1, C)  # B H' W' C\n",
    "        # self.out = x.detach().clone()#.cpu()\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CSWinBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, reso, num_heads,\n",
    "                 split_size=7, mlp_ratio=4., qkv_bias=False, qk_scale=None,\n",
    "                 drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
    "                 last_stage=False):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.patches_resolution = reso\n",
    "        self.split_size = split_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.norm1 = norm_layer(dim)\n",
    "\n",
    "        if self.patches_resolution == split_size:\n",
    "            last_stage = True\n",
    "        if last_stage:\n",
    "            self.branch_num = 1\n",
    "        else:\n",
    "            self.branch_num = 2\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(drop)\n",
    "\n",
    "        if last_stage:\n",
    "            self.attns = nn.ModuleList([\n",
    "                LePEAttention(\n",
    "                    dim, resolution=self.patches_resolution, idx=-1,\n",
    "                    split_size=split_size, num_heads=num_heads, dim_out=dim,\n",
    "                    qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "                for i in range(self.branch_num)])\n",
    "        else:\n",
    "            self.attns = nn.ModuleList([\n",
    "                LePEAttention(\n",
    "                    dim // 2, resolution=self.patches_resolution, idx=i,\n",
    "                    split_size=split_size, num_heads=num_heads // 2, dim_out=dim // 2,\n",
    "                    qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "                for i in range(self.branch_num)])\n",
    "\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, out_features=dim, act_layer=act_layer,\n",
    "                       drop=drop)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "\n",
    "        H = W = self.patches_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"flatten img_tokens has wrong size\"\n",
    "        img = self.norm1(x)\n",
    "        qkv = self.qkv(img).reshape(B, -1, 3, C).permute(2, 0, 1, 3)\n",
    "\n",
    "        if self.branch_num == 2:\n",
    "            x1 = self.attns[0](qkv[:, :, :, :C // 2])\n",
    "            x2 = self.attns[1](qkv[:, :, :, C // 2:])\n",
    "            attened_x = torch.cat([x1, x2], dim=2)\n",
    "        else:\n",
    "            attened_x = self.attns[0](qkv)\n",
    "        attened_x = self.proj(attened_x)\n",
    "        x = x + self.drop_path(attened_x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def img2windows(img, H_sp, W_sp):\n",
    "    \"\"\"\n",
    "    img: B C H W\n",
    "    \"\"\"\n",
    "    B, C, H, W = img.shape\n",
    "    img_reshape = img.reshape(B, C, H // H_sp, H_sp, W // W_sp, W_sp)\n",
    "    img_perm = img_reshape.permute(0, 2, 4, 3, 5, 1).reshape(-1, H_sp * W_sp, C)\n",
    "    return img_perm\n",
    "\n",
    "\n",
    "def windows2img(img_splits_hw, H_sp, W_sp, H, W):\n",
    "    \"\"\"\n",
    "    img_splits_hw: B' H W C\n",
    "    \"\"\"\n",
    "    B = int(img_splits_hw.shape[0] / (H * W / H_sp / W_sp))\n",
    "\n",
    "    img = img_splits_hw.reshape(B, H // H_sp, W // W_sp, H_sp, W_sp, -1)\n",
    "    img = img.permute(0, 1, 3, 2, 4, 5).reshape(B, H, W, -1)\n",
    "    return img\n",
    "\n",
    "\n",
    "class Merge_Block(nn.Module):\n",
    "    def __init__(self, dim, dim_out, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(dim, dim_out, 3, 2, 1)\n",
    "        self.norm = norm_layer(dim_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, new_HW, C = x.shape\n",
    "        H = W = int(np.sqrt(new_HW))\n",
    "        x = x.transpose(-2, -1).reshape(B, C, H, W)\n",
    "        x = self.conv(x)\n",
    "        B, C = x.shape[:2]\n",
    "        x = x.reshape(B, C, -1).transpose(-2, -1)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CSWinTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=[1000], embed_dim=96, depth=[2, 2, 6, 2],\n",
    "                 split_size=[3, 5, 7],\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=True, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm, use_chk=False,\n",
    "                 circle_loss_enabled=False, normed_linear=False, keep_norm=False, aggregate=False):\n",
    "        super().__init__()\n",
    "        self.use_chk = use_chk\n",
    "        self.head_class_counts = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.keep_norm = keep_norm\n",
    "        self.aggregate = aggregate\n",
    "        heads = num_heads\n",
    "\n",
    "        self.stage1_conv_embed = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, embed_dim, 7, 4, 2),\n",
    "            Rearrange('b c h w -> b (h w) c', h=img_size // 4, w=img_size // 4),\n",
    "            nn.LayerNorm(embed_dim)\n",
    "        )\n",
    "\n",
    "        curr_dim = embed_dim\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, np.sum(depth))]  # stochastic depth decay rule\n",
    "        self.stage1 = nn.ModuleList([\n",
    "            CSWinBlock(\n",
    "                dim=curr_dim, num_heads=heads[0], reso=img_size // 4, mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias, qk_scale=qk_scale, split_size=split_size[0],\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth[0])])\n",
    "\n",
    "        self.merge1 = Merge_Block(curr_dim, curr_dim * 2)\n",
    "        curr_dim = curr_dim * 2\n",
    "        self.stage2 = nn.ModuleList(\n",
    "            [CSWinBlock(\n",
    "                dim=curr_dim, num_heads=heads[1], reso=img_size // 8, mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias, qk_scale=qk_scale, split_size=split_size[1],\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[np.sum(depth[:1]) + i], norm_layer=norm_layer)\n",
    "                for i in range(depth[1])])\n",
    "\n",
    "        self.merge2 = Merge_Block(curr_dim, curr_dim * 2)\n",
    "\n",
    "        curr_dim = curr_dim * 2\n",
    "\n",
    "        self.stage3 = nn.ModuleList([CSWinBlock(\n",
    "            dim=curr_dim, num_heads=heads[2], reso=img_size // 16, mlp_ratio=mlp_ratio,\n",
    "            qkv_bias=qkv_bias, qk_scale=qk_scale, split_size=split_size[2],\n",
    "            drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "            drop_path=dpr[np.sum(depth[:2]) + i], norm_layer=norm_layer)\n",
    "            for i in range(depth[2])])\n",
    "\n",
    "        self.final_heads = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                Merge_Block(curr_dim, curr_dim * 2),\n",
    "                nn.Sequential(\n",
    "                    *[CSWinBlock(\n",
    "                        dim=curr_dim * 2, num_heads=heads[3], reso=img_size // 32, mlp_ratio=mlp_ratio,\n",
    "                        qkv_bias=qkv_bias, qk_scale=qk_scale, split_size=split_size[-1],\n",
    "                        drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                        drop_path=dpr[np.sum(depth[:-1]) + i], norm_layer=norm_layer, last_stage=True)\n",
    "                        for i in range(depth[-1])]),\n",
    "                norm_layer(curr_dim * 2),\n",
    "                nn.Linear(curr_dim * 2, head_class_count)\n",
    "            ])\n",
    "            for head_class_count in self.head_class_counts])\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm2d)):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token'}\n",
    "\n",
    "    def get_classifier(self):\n",
    "        return self.head\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        if self.num_classes != num_classes:\n",
    "            print('reset head to', num_classes)\n",
    "            self.num_classes = num_classes\n",
    "            self.head = nn.Linear(self.out_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "            self.head = self.head.cuda()\n",
    "            trunc_normal_(self.head.weight, std=.02)\n",
    "            if self.head.bias is not None:\n",
    "                nn.init.constant_(self.head.bias, 0)\n",
    "\n",
    "    def forward_backbone(self, x):\n",
    "        for pre, blocks in zip([self.merge1, self.merge2],\n",
    "                               [self.stage2, self.stage3]):\n",
    "            x = pre(x)\n",
    "            for blk in blocks:\n",
    "                if self.use_chk:\n",
    "                    x = checkpoint.checkpoint(blk, x)\n",
    "                else:\n",
    "                    x = blk(x)\n",
    "        return x\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.stage1_conv_embed(x)\n",
    "\n",
    "        for blk in self.stage1:\n",
    "            if self.use_chk:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "\n",
    "        x = self.forward_backbone(x)\n",
    "\n",
    "        results = []\n",
    "        for head_idx, head_layer in enumerate(self.final_heads):\n",
    "            merge, stage, norm, head = head_layer\n",
    "\n",
    "            head_x = merge(x)\n",
    "            head_x = stage(head_x)\n",
    "            head_x = norm(head_x)\n",
    "            head_x = torch.mean(head_x, dim=1)\n",
    "            head_x = head(head_x)\n",
    "            results += [head_x]\n",
    "\n",
    "        return results\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        return x\n",
    "\n",
    "    def get_last_shared_params(self):\n",
    "        return self.stage3.parameters()\n",
    "\n",
    "\n",
    "def _conv_filter(state_dict, patch_size=16):\n",
    "    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n",
    "    out_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if 'patch_embed.proj.weight' in k:\n",
    "            v = v.reshape((v.shape[0], 3, patch_size, patch_size))\n",
    "        out_dict[k] = v\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "### 224 models\n",
    "\n",
    "\n",
    "@register_model\n",
    "def cswin_tiny_224_min(pretrained=False, pretrained_cfg='', head_class_counts=[10, 10], bn_tf=False, **kwargs):\n",
    "    model = CSWinTransformer(patch_size=4, embed_dim=64, depth=[1, 2, 21, 1],\n",
    "                             split_size=[1, 2, 7, 7], num_heads=[2, 4, 8, 16], mlp_ratio=4.,\n",
    "                             num_classes=head_class_counts, **kwargs)\n",
    "    model.default_cfg = default_cfgs['cswin_224']\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Env setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "num_epochs = 10\n",
    "alpha = 1.5\n",
    "head_class_counts = [10, 10]\n",
    "num_tasks = 2\n",
    "# num_tasks = len(head_class_counts)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_chk = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model('cswin_tiny_224_min', head_class_counts=head_class_counts, use_chk=use_chk).to(device)\n",
    "lambda_weights = torch.ones((num_tasks, ), requires_grad=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "model_optim = optim.AdamW(model.parameters(), lr=0.01)\n",
    "lambda_optim = optim.AdamW([lambda_weights])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size=224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Download and load the MNIST training set\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_dataset = Subset(train_dataset, range(300)) # just for the example to get faster results\n",
    "\n",
    "# Create a DataLoader for the training set\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Download and load the MNIST test set\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_dataset = Subset(test_dataset, range(300))\n",
    "\n",
    "# Create a DataLoader for the test set\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/skal/dev/ml/logmeal/min_setup.ipynb Cell 13\u001b[0m in \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/skal/dev/ml/logmeal/min_setup.ipynb#X22sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# FAILURE POINT\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/skal/dev/ml/logmeal/min_setup.ipynb#X22sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mfor\u001b[39;00m w_i, l_i \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(lambda_weights, loss_tasks):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/skal/dev/ml/logmeal/min_setup.ipynb#X22sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     local_grad \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgrad(l_i, model\u001b[39m.\u001b[39mstage3\u001b[39m.\u001b[39mparameters(), retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/skal/dev/ml/logmeal/min_setup.ipynb#X22sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     norms\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mnorm(w_i \u001b[39m*\u001b[39m local_grad))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/skal/dev/ml/logmeal/min_setup.ipynb#X22sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m norms \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(norms)\n",
      "File \u001b[0;32m~/dev/ml/logmeal/venv_cs_hydra/lib/python3.8/site-packages/torch/autograd/__init__.py:275\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs)\n\u001b[1;32m    274\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 275\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    276\u001b[0m         outputs, grad_outputs_, retain_graph, create_graph, inputs,\n\u001b[1;32m    277\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior."
     ]
    }
   ],
   "source": [
    "initial_loss = None\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss_a = 0.0\n",
    "    running_loss_b = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    loss_m = AverageMeter()\n",
    "    loss_a_m = AverageMeter()\n",
    "    loss_b_m = AverageMeter()\n",
    "    weights_a_m = AverageMeter()\n",
    "    weights_b_m = AverageMeter()\n",
    "    model.train()\n",
    "\n",
    "    for idx, (img, target) in enumerate(train_dataloader):\n",
    "        img = img.to(device)\n",
    "        target = target.to(device)\n",
    "        target_b = torch.flip(target, [0])\n",
    "\n",
    "        model_optim.zero_grad()\n",
    "        lambda_optim.zero_grad()\n",
    "\n",
    "        out_a, out_b= model(img)\n",
    "\n",
    "        loss_a = loss_fn(out_a, target)\n",
    "        loss_b = loss_fn(out_b, target_b)\n",
    "\n",
    "        loss_tasks = torch.stack([loss_a, loss_b])\n",
    "\n",
    "        weighted_loss = lambda_weights * loss_tasks\n",
    "        if initial_loss is None:\n",
    "            initial_loss = weighted_loss\n",
    "\n",
    "        loss = weighted_loss.sum()\n",
    "\n",
    "        # lambda_weights.retain_grad()\n",
    "        loss_tasks.retain_grad()\n",
    "\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        norms = []\n",
    "        # FAILURE POINT\n",
    "        for w_i, l_i in zip(lambda_weights, loss_tasks):\n",
    "            local_grad = torch.autograd.grad(l_i, model.stage3.parameters(), retain_graph=True)[0]\n",
    "            norms.append(torch.norm(w_i * local_grad))\n",
    "\n",
    "        norms = torch.stack(norms)\n",
    "        nw = norms.mean()\n",
    "        with torch.no_grad():\n",
    "            # loss ratios\n",
    "            loss_ratios = loss_tasks / initial_loss\n",
    "            # inverse training rate r(t)\n",
    "            inverse_train_rates = loss_ratios / loss_ratios.mean()\n",
    "            constant_term =  nw * (inverse_train_rates ** alpha)\n",
    "\n",
    "        # compute Lgrad\n",
    "        lgrad = (norms - constant_term).abs().sum()\n",
    "        tgrad = torch.autograd.grad(lgrad, lambda_weights)\n",
    "        lambda_weights.grad  = tgrad[0]\n",
    "\n",
    "        model_optim.step()\n",
    "        lambda_optim.step()\n",
    "\n",
    "        # Renormalize\n",
    "        with torch.no_grad():\n",
    "            renormalize = num_tasks / lambda_weights.sum()\n",
    "            lambda_weights *= renormalize\n",
    "\n",
    "        weights_a_m.update(lambda_weights[0].item(), lambda_weights.size(0))\n",
    "        weights_b_m.update(lambda_weights[1].item(), lambda_weights.size(0))\n",
    "        loss_m.update(loss.item(), img.size(0))\n",
    "        loss_a_m.update(loss_a.item(), img.size(0))\n",
    "        loss_b_m.update(loss_b.item(), img.size(0))\n",
    "        running_loss_a += loss_a.item()\n",
    "        running_loss_b += loss_b.item()\n",
    "        _, predicted = torch.max(out_a.data, 1)\n",
    "        total_samples += target.size(0)\n",
    "        correct_predictions += (predicted == target).sum().item()\n",
    "\n",
    "    # Calculate average loss and accuracy for the epoch\n",
    "    epoch_loss_a = running_loss_a / len(train_dataloader)\n",
    "    epoch_loss_b = running_loss_b / len(train_dataloader)\n",
    "    epoch_accuracy = correct_predictions / total_samples\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss_a: {epoch_loss_a:.4f}, Loss_b: {epoch_loss_b:.4f}, W1: {weights_a_m.avg} W2: {weights_b_m.avg} Accuracy: {epoch_accuracy:.4f}')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_cs_hydra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
